# 从优化建模的角度阐述SVM的前世今生

## 问题


$\min\limits_{w,b,\xi_k(\forall k)}\frac12\|w\|_2^2+C\sum_{k=1}^n\xi_k$

$s.t.\ y_k(w^Tx_k+b)\geq1-\xi_k(k=1,2,\ldots,n)$

$\xi_k\geq0(k=1,2,\ldots,n)$

## 介绍

支撑向量机(support vector machine)，简称SVM，是一类按监督学习方式对数据进行二元分类的广义线性分类器，其决策边界是对学习样本求解的最大边距超平面。其被提出于1964年，在二十世纪90年代后得到快速发展并衍生出一系列改进和扩展算法，在人像识别、文本分类等模式识别问题中有得到应用。

给定一些数据点，它们分属于两个不同的类，现在想要找到一个线性分类器将其分成两类。我们将数据点集合映射到空间中的向量集合$X=\{x_1,x_2,...,x_m\}$, 用 $y$ 表示类别(取$1$或$-1$表示两个不同的类)。 我们希望基于$X$的空间特征进行分类， 希望在这个空间中找到一个超平面$H=\{z:w^Tz=-b\}$将数据点分割成两组，其中$w,b$为待定系数，$w$是垂直于超平面的一个向量。 令$f(x) = w^Tz + b$, 当$f(x) = 0$时，$x$是位于超平面上的点， 当$f(x)>0$时，$x$对应于$y=1$的数据点， 当$f(x)<0$时，$x$对应于$y=-1$的数据点。 而在$w,b$确定时， $|f(x)|$表示数据点$x$到超平面的远近。 当超平面离数据点的间隔越大， 分类的确信度(confidence)就越大。 因此，为了找到一个尽量优秀的超平面将数据点分类， 需要找到使得样本点到超平面的最小距离最大的超平面。 SVM就是要求得这样的一个超平面。

由于在超平面固定后，可以等比例缩放$w,b$的大小，因此$f(x)$的值可以任意大，即函数间隔可以取无限大。 而几何间隔由于除上了$\|w\|$,只会随着超平面变动而变动。 考虑离超平面最近的两个点$x_1,x_2$，显然二者离超平面的距离可以也应相等，令其分别位于$w^Tx_1+b=1,w^Tx_1+b=-1$上，两个平面的距离为$\frac2{\|w\|_2}$。


变换目标函数，得到如下优化问题：

$\min\limits_{w,b}\frac12\|w\|_2^2$

$s.t.\ y_i(w^Tx_i+b)\geq1, i=1,...,n$

此时的目标函数是二次的，约束条件是线性的，是一个凸二次规划问题。由于问题的特殊结构，借由拉格朗日对偶性， 可以通过对偶问题得到原问题的最优解。

另外，样本不一定是线性可分的，即不存在$w,b$使得 $ y_i(w^Tx_i+b)\geq1, i=1,...,n$。
此时引入松弛变量$\xi_k\geq0$和惩罚因子$C$，使存在$w,b$满足 $y_k(w^Tx_k+b)\geq1-\xi_k(k=1,2,\ldots,n)$，此时则将问题改写如下：

$\min\limits_{w,b,\xi_k(\forall k)}\frac12\|w\|_2^2+C\sum_{k=1}^n\xi_k$

$s.t.\ y_k(w^Tx_k+b)\geq1-\xi_k(k=1,2,\ldots,n)$

$\xi_k\geq0(k=1,2,\ldots,n)$




## SVM优化问题性质与求解


$\min\limits_{w,b,\xi_k(\forall k)}\frac12\|w\|_2^2+C\sum_{k=1}^n\xi_k$

目标函数是二次的，约束条件是仿射形式，SVM优化问题是一个凸二次规划问题。

如果令$w=b=0,\xi_k=2$，则有$y_k(w^Tx_k+b)=0>1-\xi_k=1-2=-1$，不等式条件严格满足，SVM优化问题满足Slater条件。

SVM凸优化问题强对偶性成立，对偶间隙$p^*-d^*=0$，原问题与对偶问题有相同的最优解，因此可以转化为对偶问题求解。

推导得到拉格朗日函数：$L(w,b,\xi,\alpha,\beta)=\frac12\|w\|_2^2+\sum_{i=1}^n(C\xi_i-\alpha_i(y_i(w^Tx_i+b)-1+\xi_i)-\beta_i\xi_i)$

进而得到拉格朗日对偶函数$g(\alpha,\beta)=\min\limits_{w,b,\xi}L(w,b,\xi,\alpha,\beta)$

对$w,b,\xi$求导为0
解得$w=\sum_{i=1}^n\alpha_ix_iy_i$,  $\sum_{i=1}^n\alpha_iy_i=0 $,  $\alpha_i+\beta_i=C$

则$g(\alpha,\beta)=g'(\alpha)=\frac12\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jx_i^Tx_jy_iy_j-\sum_{i=1}^n\alpha_i$

对偶问题为：

$\min \frac12\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jx_i^Tx_jy_iy_j-\sum_{i=1}^n\alpha_i$

$s.t.\ 0\leq \alpha_i\leq C,i = 1,2,...,n,\sum_{i=1}^n\alpha_iy_i=0$

求解对偶问题即可。


## 一些关于SVM求解问题的今生拓展

### 核函数

大部分数据并非线性可分的，这时候满足上述条件的超平面不存在。对于非线性数据，SVM的处理方法是选择核函数，通过将数据映射到高维空间，解决数据在原空间线性不可分的问题。向量机首先在原始空间中完成计算，再通过核函数将输入空间映射到高维特征空间，并在高维特征空间中构造出超平面，从而将数据分开。

不考虑核函数，我们先用线性学习器学习一个非线性关系，选择出一个非线性特征集。等价与应用一个非线性映射，将数据映射到特征空间，并在特征空间中使用线性学习器，考虑的假设集应是该类函数：$f(x)=\sum_{i=1}w_i\Phi_i(x)+b$,$\Phi：X$->$F$是从输入空间到某个特征空间的映射。
由于对偶性质，假设可以表达为训练点的线性集合，则决策规则可以表示为$f(x)=\sum_{i=1}\alpha_iy_i$<$\Phi_i(x)\cdot\Phi(x)$>$+b$。
若有方法，能够在特征空间直接计算内积，就有可能将两个步骤结合建立一个非线性的学习器，这种方法称作核函数方法。核函数$\kappa$是这样一个函数，能够使所有$x,z\in X$,满足$\kappa(x,z) = $<$\Phi(x)\cdot\Phi(z)$>。
通过核函数，我们可以避开再高维空间中直接进行计算，但结果仍然等价。

通常会从几个常用的核函数中选择使用，有多项式核、高斯核、线性核等。
 
### 多分类SVM

多分类SVM是一种用于处理多类别分类问题的算法。它有两种基本策略：

**一对一(OvO)策略：**
针对两个类别构造一个二分类SVM分类器，如果有$N$个类别，则构造$N(N-1)/2$个分类器，每个SVM对应两类的关系，统计每个类别的投票数，最终票数最多的即为该样本的预测类别。

**一对多(OvR)策略：**
针对每个类别，将该类别视正类，其余类别视作负类，构造一个二分类SVM分类器，若有$N$个类别则构造$N$个,对于一个新样本，通过对所有分类器分类，选择得分最高的分类器对应的类别作为样本的预测类别。

### 软间隔SVM

硬间隔SVM只适用于数据完全可分的情况，不允许间隔边界之间有任何数据点，在实际应用中往往难以实现。
软间隔SVM引入了松弛变量和惩罚参数C。松弛变量允许某些数据点违反原有的分类约束，即允许这些数据点位于间隔边界的错误一侧，甚至是错误分类。惩罚参数C则用于控制这种违反约束的程度，即对误分类点的惩罚力度。
C的值越大，模型对误分类的惩罚越重，倾向于尽可能地将所有点正确分类，这可能导致模型在训练集上表现很好，但在测试集上泛化能力较差（过拟合）。相反，C的值越小，模型对误分类的惩罚越轻，允许更多的点违反约束，这可能会提高模型的泛化能力，但也可能导致模型在训练集上表现不佳（欠拟合）。
软间隔SVM的优化目标是在最大化间隔和最小化误分类点的数量（或误分类的程度）之间找到一个平衡。这通常通过求解一个带有约束的二次规划问题来实现。在求解过程中，会找到一组支持向量，这些向量是距离超平面最近的点，它们对于确定超平面的位置至关重要。与硬间隔SVM不同，软间隔SVM中的支持向量可能包括一些位于间隔边界错误一侧的点。




